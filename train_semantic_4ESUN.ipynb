{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e2326b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "import csv\n",
    "# TSNE test\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "# TSNE test\n",
    "import torch.nn.functional as F # test\n",
    "\n",
    "from capsule_network_semantic import CapsuleNetwork\n",
    "from data_preprocess_semantic import DataPreprocess\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "from transformers import BertModel, AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "import utils\n",
    "import log\n",
    "import collections\n",
    "\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "\n",
    "parser = ArgumentParser()\n",
    "# for dataset\n",
    "parser.add_argument('--train_data_path', default='./data/SNIPS/sample/train_split.csv', type=str, help=\"path of training data\")\n",
    "parser.add_argument('--test_data_path', default='./data/SNIPS/sample/test_split.csv', type=str, help=\"path of testing data\")\n",
    "parser.add_argument('--train_class_path', default='./data/SNIPS/class/train_classes.txt', type=str, help=\"path of training classes\")\n",
    "parser.add_argument('--test_class_path', default='./data/SNIPS/class/test_classes.txt', type=str, help=\"path of testing classes\")\n",
    "parser.add_argument('--train_description_path', default='./data/SNIPS/description/train_description_2.txt', type=str, help=\"path of train descriptions\")\n",
    "parser.add_argument('--test_description_path', default='./data/SNIPS/description/test_description_2.txt', type=str, help=\"path of test descriptions\")\n",
    "parser.add_argument('--w2v_path', default='./data/wiki.en.vec', type=str, help=\"path of pretrained w2v\")\n",
    "# for training\n",
    "parser.add_argument('--use_gpu', default=True, type=bool, help=\"To use GPU or not\")\n",
    "parser.add_argument('--epochs', default=100, type=int, help=\"number of epochs for training\")\n",
    "parser.add_argument('--batch_size', default=512, type=int, help=\"batch size for training\")\n",
    "parser.add_argument('--learning_rate', default=0.001, type=float, help=\"learning rate for training\")\n",
    "parser.add_argument('--drop', default=0.5, type=float, help=\"dropout rate for self-attention\")\n",
    "# for self-attention\n",
    "parser.add_argument('--attention_mode', default=\"dimensional\", type=str, choices=[\"normal\", \"dimensional\"], help=\"mode of self-attention\")\n",
    "parser.add_argument('--d_a', default=10, type=int, help=\"hidden unit number of self-attention\")\n",
    "parser.add_argument('--r', default=3, type=int, help=\"number of self-attention heads\")\n",
    "# for capsule\n",
    "#parser.add_argument('--n_seen', type=int, required=True, help=\"number of seen classes\")\n",
    "#parser.add_argument('--n_unseen', type=int, required=True, help=\"number of unseen classes\")\n",
    "parser.add_argument('--d_p', default=10, type=int, help=\"dimention of prediction vector\")\n",
    "# for routing\n",
    "parser.add_argument('--routing_iter', default=3, type=int, help=\"iterations for dynamic routing\")\n",
    "# for loss\n",
    "parser.add_argument('--alpha', default=0.001, type=float, help=\"coefficient of self-attention loss\")\n",
    "# for testing/inference\n",
    "parser.add_argument('--similarity_mode', default=\"description\", type=str, choices=[\"w2v\", \"BERT\", \"S_BERT\", \"description\"], help=\"mode of similarity computation\")\n",
    "parser.add_argument('--sigma', default=0.2, type=float, help=\"scale for similarity\")\n",
    "parser.add_argument('--eval_mode', default=\"best\", type=str, choices=[\"normal\", \"unseen_first\", \"avg\", \"avg_2stage\", \"avg_logits\", \"best\"], help=\"mode of evaluation\")\n",
    "# for LOF\n",
    "parser.add_argument('--use_LOF', default=False, type=bool, help=\"To use LOF prediction for testing or not\")\n",
    "parser.add_argument('--n_neighbors', default=20, type=int, help=\"number of nearest neighbors, i.e. MinPts in the paper\")\n",
    "#parser.add_argument('--contamination', default=0.2, type=float, help=\"contamination of LOF\")\n",
    "# for saveing model\n",
    "parser.add_argument('--save_path', default='./saved_models/', type=str, help=\"path of saved models\")\n",
    "parser.add_argument('--save_per_epoch', default=100, type=int, help=\"save model per how many epochs\")\n",
    "# for loading model\n",
    "parser.add_argument('--checkpoint_path', type=str, help=\"path of checkpoint to load\")\n",
    "# for saving log\n",
    "parser.add_argument('--args_path', default='./logs/args.json', type=str, help=\"path of saved args\")\n",
    "parser.add_argument('--log_path', default='./logs/log.json', type=str, help=\"path of saved log\")\n",
    "\n",
    "args, unknown = parser.parse_known_args()\n",
    "#args = parser.parse_args()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.use_gpu else \"cpu\")\n",
    "\n",
    "def test(epoch, model, test_loader, similarity, n_seen, n_unseen, log_dict, description_test, score_threshold, score_threshold_norm, train_represent, evaluation_mode):\n",
    "    # generalizer zero-shot classification\n",
    "    print(\"Testing Epoch {}...\\n\".format(epoch))\n",
    "    model.eval()\n",
    "    start_time = time.time()\n",
    "    test_pred = torch.LongTensor([])\n",
    "    test_target = torch.LongTensor([])\n",
    "    test_logits = torch.DoubleTensor([])\n",
    "    class_scores = torch.DoubleTensor([])\n",
    "    class_scores_norm = torch.DoubleTensor([])\n",
    "    n_all = n_seen + n_unseen\n",
    "    seen_des_features = train_represent.to(device) # [n_seen, r, hidden_size]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for input_embeddings, onehot_label, sentence_embeddings in test_loader:\n",
    "            # model inference\n",
    "            model.construct_unseen_capsule_weights(similarity)\n",
    "            \n",
    "            model.inference(input_embeddings.to(device), description_test.to(device))\n",
    "            \n",
    "            test_logits_batch = model.logits.cpu()\n",
    "            test_logits = torch.cat((test_logits, test_logits_batch))\n",
    "                             \n",
    "            pred = torch.argmax(test_logits_batch, dim=1).cpu() \n",
    "            target = torch.argmax(onehot_label, dim=1).cpu()\n",
    "            \n",
    "            test_pred = torch.cat((test_pred, pred))\n",
    "            test_target = torch.cat((test_target, target))\n",
    "            \n",
    "            input_semantics = torch.unsqueeze(model.semantic_features, 1).repeat(1, n_all, 1, 1) # [batch_size, 7, r, hidden_size]\n",
    "            \n",
    "            #des_features = model.description_features # [7, r, hidden_size]\n",
    "            \n",
    "            unseen_des_features = model.description_features[n_seen:] # [n_unseen, r, hidden_size]\n",
    "            des_features = torch.cat((seen_des_features, unseen_des_features))\n",
    "            \n",
    "            semantic_scores = torch.sum(input_semantics * des_features, dim=-1) # [batch_size, 7, r]\n",
    "            len_input_sen = torch.norm(input_semantics, dim=3) # [batch_size, 7, r]\n",
    "            len_des = torch.norm(des_features, dim=2) # [7, r]\n",
    "            \n",
    "            semantic_scores_norm = semantic_scores / (len_input_sen * len_des) # [batch_size, 7, r]\n",
    "\n",
    "            class_scores_batch = torch.sum(semantic_scores, dim=-1).cpu() # [batch_size, 7]\n",
    "            class_scores_norm_batch = torch.sum(semantic_scores_norm, dim=-1).cpu() # [batch_size, 7]\n",
    "            \n",
    "            class_scores = torch.cat((class_scores, class_scores_batch))\n",
    "            class_scores_norm = torch.cat((class_scores_norm, class_scores_norm_batch))\n",
    "\n",
    "    \n",
    "    argmax_logits = np.zeros((n_all, n_all), dtype=int)\n",
    "    argmax_norm_logits = np.zeros((n_all, n_all), dtype=int)\n",
    "    \n",
    "    \n",
    "    preds = [0] * n_all\n",
    "    preds_correct = [0] * n_all\n",
    "    \n",
    "    preds_norm = [0] * n_all\n",
    "    preds_norm_correct = [0] * n_all\n",
    "    \n",
    "    test_pred_norm = torch.clone(test_pred)\n",
    "    \n",
    "    \n",
    "    correct_point = [0, 0, 0, 0, 0]\n",
    "    wrong_point = [0, 0, 0, 0, 0]\n",
    "    correct_point_norm = [0, 0, 0, 0, 0]\n",
    "    wrong_point_norm = [0, 0, 0, 0, 0]\n",
    "    \n",
    "    \n",
    "    for i in range(len(test_target)):\n",
    "        #'''\n",
    "        argmax_class_score = torch.argmax(class_scores[i])\n",
    "        argmax_class_score_norm = torch.argmax(class_scores_norm[i])\n",
    "        max_class_logits = torch.argmax(test_logits[i])\n",
    "        #'''\n",
    "        \n",
    "        if evaluation_mode == 'normal':\n",
    "            pass\n",
    "        elif evaluation_mode == 'unseen_first':\n",
    "            if argmax_class_score >= n_seen:\n",
    "                test_pred[i] = argmax_class_score\n",
    "\n",
    "            if argmax_class_score_norm >= n_seen:\n",
    "                test_pred_norm[i] = argmax_class_score_norm\n",
    "        \n",
    "        elif evaluation_mode == 'avg':\n",
    "            if class_scores[i][test_pred[i]] < score_threshold[test_pred[i]]:\n",
    "                unseen_class_scores = torch.clone(class_scores[i])\n",
    "                unseen_class_scores[:n_seen] = 0\n",
    "                test_pred[i] = torch.argmax(unseen_class_scores)\n",
    "\n",
    "            if class_scores_norm[i][test_pred_norm[i]] < score_threshold_norm[test_pred_norm[i]]:\n",
    "                unseen_class_scores_norm = torch.clone(class_scores_norm[i])\n",
    "                unseen_class_scores_norm[:n_seen] = 0\n",
    "                test_pred_norm[i] = torch.argmax(unseen_class_scores_norm)\n",
    "        \n",
    "        elif evaluation_mode == 'avg_2stage':\n",
    "            if test_pred[i] != argmax_class_score:\n",
    "                if class_scores[i][test_pred[i]] < score_threshold[test_pred[i]]:\n",
    "                    unseen_class_scores = torch.clone(class_scores[i])\n",
    "                    unseen_class_scores[:n_seen] = 0\n",
    "                    test_pred[i] = torch.argmax(unseen_class_scores)\n",
    "                    # [1]: score of pred < threshold -> unseen\n",
    "                    if test_pred[i] == test_target[i]:\n",
    "                        correct_point[1] += 1\n",
    "                    else:\n",
    "                        wrong_point[1] += 1\n",
    "            else:\n",
    "                # [0]: pred == argmax score\n",
    "                if test_pred[i] == test_target[i]:\n",
    "                    correct_point[0] += 1\n",
    "                else:\n",
    "                    wrong_point[0] += 1\n",
    "\n",
    "\n",
    "            if test_pred_norm[i] != argmax_class_score_norm:\n",
    "                if class_scores_norm[i][test_pred_norm[i]] < score_threshold_norm[test_pred_norm[i]]:\n",
    "                    unseen_class_scores = torch.clone(class_scores_norm[i])\n",
    "                    unseen_class_scores[:n_seen] = 0\n",
    "                    test_pred_norm[i] = torch.argmax(unseen_class_scores)\n",
    "                    # [1]: score of pred < threshold -> unseen\n",
    "                    if test_pred_norm[i] == test_target[i]:\n",
    "                        correct_point_norm[1] += 1\n",
    "                    else:\n",
    "                        wrong_point_norm[1] += 1\n",
    "            else:\n",
    "                # [0]: pred == argmax score\n",
    "                if test_pred_norm[i] == test_target[i]:\n",
    "                    correct_point_norm[0] += 1\n",
    "                else:\n",
    "                    wrong_point_norm[0] += 1\n",
    "                    \n",
    "        elif evaluation_mode == 'avg_logits':\n",
    "            if test_pred[i] != argmax_class_score:\n",
    "                if test_logits[i][max_class_logits] < 0.9:\n",
    "                    unseen_class_scores = torch.clone(class_scores[i])\n",
    "                    unseen_class_scores[:n_seen] = 0\n",
    "                    test_pred[i] = torch.argmax(unseen_class_scores)\n",
    "                    # [1]: max logits < 0.9 -> unseen\n",
    "                    if test_pred[i] == test_target[i]:\n",
    "                        correct_point[1] += 1\n",
    "                    else:\n",
    "                        wrong_point[1] += 1\n",
    "                else:\n",
    "                    if argmax_class_score >= n_seen:\n",
    "                        test_pred[i] = argmax_class_score\n",
    "                        # [2]: argmax class unseen -> unseen\n",
    "                        if test_pred[i] == test_target[i]:\n",
    "                            correct_point[2] += 1\n",
    "                        else:\n",
    "                            wrong_point[2] += 1\n",
    "                    else:\n",
    "                        # [3]: argmax class seen\n",
    "                        if test_pred[i] == test_target[i]:\n",
    "                            correct_point[3] += 1\n",
    "                        else:\n",
    "                            wrong_point[3] += 1\n",
    "            else:\n",
    "                # [0]: pred == argmax score\n",
    "                if test_pred[i] == test_target[i]:\n",
    "                    correct_point[0] += 1\n",
    "                else:\n",
    "                    wrong_point[0] += 1\n",
    "                    \n",
    "        elif evaluation_mode == 'best':\n",
    "            if test_pred[i] != argmax_class_score:\n",
    "                if argmax_class_score >= n_seen:\n",
    "                    test_pred[i] = argmax_class_score\n",
    "                else:\n",
    "                    if test_logits[i][max_class_logits] >= 0.9:\n",
    "                        pass\n",
    "                    else:\n",
    "                        if class_scores[i][test_pred[i]] >= score_threshold[test_pred[i]]:\n",
    "                            pass\n",
    "                        else:\n",
    "                            unseen_class_scores = torch.clone(class_scores[i])\n",
    "                            unseen_class_scores[:n_seen] = 0\n",
    "                            test_pred[i] = torch.argmax(unseen_class_scores)\n",
    "            else:\n",
    "                pass\n",
    "                \n",
    "        \n",
    "        preds[argmax_class_score] += 1\n",
    "        if argmax_class_score == test_target[i]:\n",
    "            preds_correct[argmax_class_score] += 1\n",
    "        \n",
    "        preds_norm[argmax_class_score_norm] += 1\n",
    "        if argmax_class_score_norm == test_target[i]:\n",
    "            preds_norm_correct[argmax_class_score_norm] += 1\n",
    "            \n",
    "            \n",
    "        #'''\n",
    "        argmax_logits[test_target[i]][test_pred[i]] += 1\n",
    "        argmax_norm_logits[test_target[i]][test_pred_norm[i]] += 1\n",
    "        #'''\n",
    "        \n",
    "    print(\"Correct Point:\", correct_point)\n",
    "    print(\"Wrong Point:\", wrong_point, \"\\n\")\n",
    "    \n",
    "    print(\"Correct Point Norm:\", correct_point_norm)\n",
    "    print(\"Wrong Point Norm:\", wrong_point_norm, \"\\n\")\n",
    "        \n",
    "    print(\"Preds:\", preds)\n",
    "    print(\"Preds Correct:\", preds_correct)\n",
    "    print(\"Total:\", sum(preds_correct), \"\\n\")\n",
    "    \n",
    "    print(\"Preds_Norm:\", preds_norm)\n",
    "    print(\"Preds_Norm Correct:\", preds_norm_correct)\n",
    "    print(\"Total_Norm:\", sum(preds_norm_correct), \"\\n\")\n",
    "    \n",
    "    \n",
    "    #'''\n",
    "    final_correct = 0\n",
    "    final_norm_correct = 0\n",
    "    for i in range(n_all):\n",
    "        print(\"Class {}: {}\".format(i, argmax_logits[i]))\n",
    "        final_correct += argmax_logits[i][i]\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    for i in range(n_all):\n",
    "        print(\"Class Norm {}: {}\".format(i, argmax_norm_logits[i]))\n",
    "        final_norm_correct += argmax_norm_logits[i][i]\n",
    "        \n",
    "    print(\"\\n\")\n",
    "    print(\"Final Correct:\", final_correct)\n",
    "    print(\"Final Norm Correct:\", final_norm_correct)\n",
    "    #'''\n",
    "    \n",
    "    #'''\n",
    "    LOF_pred = torch.zeros(6880) # useless\n",
    "    LOF_pred = LOF_pred + 1      # useless\n",
    "    log_dict, acc = log.fill_log(log_dict, LOF_pred, test_target, test_pred, n_seen)        \n",
    "    \n",
    "    print(classification_report(test_target, test_pred, digits=4, zero_division=1))\n",
    "    #'''\n",
    "    \n",
    "    test_epoch_time = time.time() - start_time\n",
    "    \n",
    "    norm_acc = accuracy_score(test_pred_norm, test_target)\n",
    "    print(\"norm_acc: {}\".format(norm_acc))\n",
    "    \n",
    "    return acc, test_epoch_time\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc18f71",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    # handle dataset\n",
    "    BERT_tokenizer = AutoTokenizer.from_pretrained('./bert-base-uncased', local_files_only=True)\n",
    "    BERT_model = BertModel.from_pretrained('./bert-base-uncased', local_files_only=True)\n",
    "    BERT_model.eval()\n",
    "    sentence_BERT = SentenceTransformer('./paraphrase-distilroberta-base-v2')\n",
    "\n",
    "    train_set = DataPreprocess(BERT_model, BERT_tokenizer, sentence_BERT, args.w2v_path, args.train_data_path, args.train_class_path, args.train_description_path, 'train')\n",
    "    test_set = DataPreprocess(BERT_model, BERT_tokenizer, sentence_BERT, args.w2v_path, args.test_data_path, args.test_class_path, args.test_description_path, 'test')\n",
    "    print('Data Preprocess end!\\n\\n')\n",
    "    \n",
    "    n_seen = len(train_set.class_index) # number of seen_classes\n",
    "    n_all_classes = len(test_set.class_index) # number of all_classes\n",
    "    n_unseen = n_all_classes - n_seen # number of unseen_classes\n",
    "    \n",
    "    similarity = torch.from_numpy(utils.get_sim(train_set, test_set, n_seen, args.sigma, args.similarity_mode)).float().to(device) # test_set contains seen and unseen labels, [n_unseen, n_seen]\n",
    "    \n",
    "    train_loader = DataLoader(train_set, batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
    "    test_loader = DataLoader(test_set, batch_size=args.batch_size, shuffle=True, num_workers=4)\n",
    "        \n",
    "    # create model\n",
    "    model = CapsuleNetwork(args, n_seen, n_unseen, device).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.learning_rate)\n",
    "    \n",
    "    # learning rate scheduler\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.5)\n",
    "    \n",
    "    if not os.path.isdir(args.save_path):\n",
    "        os.mkdir(args.save_path)\n",
    "        \n",
    "    # checkpoint\n",
    "    if args.checkpoint_path:\n",
    "        print(\"Loading checkpoint from {}\".format(args.checkpoint_path))\n",
    "        model.load_state_dict(torch.load(args.checkpoint_path))\n",
    "    else:\n",
    "        print(\"Initializing Variables (no checkpoint)\\n\")\n",
    "        \n",
    "    # prepare log\n",
    "    log_dict = log.prepare_log(similarity)\n",
    "    \n",
    "        \n",
    "    # train\n",
    "    train_time = 0\n",
    "    test_time = 0\n",
    "    best_acc = 0\n",
    "\n",
    "    for epoch in range(1, args.epochs + 1):\n",
    "        \n",
    "        print(\"****************************************\\nTraining Epoch {}...\\n\".format(epoch))\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        start_time = time.time()\n",
    "        train_pred = torch.LongTensor([])\n",
    "        train_target = torch.LongTensor([])\n",
    "        \n",
    "        # for semantic score threshold\n",
    "        score_threshold = [0] * n_seen\n",
    "        score_threshold_norm = [0] * n_seen\n",
    "        target_count = [0] * n_seen\n",
    "        \n",
    "        # for train represent\n",
    "        train_represent = torch.zeros(n_seen, args.r, 768)\n",
    "        target_count = [0] * n_seen\n",
    "        \n",
    "        for input_embeddings, onehot_label, sentence_embeddings in train_loader: # batch\n",
    "            # model update\n",
    "            model(input_embeddings.to(device), train_set.description_test.to(device))\n",
    "            \n",
    "            loss = model.loss(onehot_label.to(device).float())\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            clip_grad_norm_(model.parameters(), 1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            # train prediction\n",
    "            pred = torch.argmax(model.logits, dim=1).cpu()\n",
    "            target = torch.argmax(onehot_label, dim=1).cpu()\n",
    "\n",
    "            batch_acc = accuracy_score(target, pred)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            train_pred = torch.cat((train_pred, pred))\n",
    "            train_target = torch.cat((train_target, target))\n",
    "            \n",
    "            #''' for semantic score\n",
    "            input_semantics = torch.unsqueeze(model.semantic_features.detach(), 1).repeat(1, n_seen, 1, 1) # [batch_size, 5, r, hidden_size]\n",
    "            des_features = model.description_features.detach() # [5, r, hidden_size]\n",
    "            \n",
    "            semantic_scores = torch.sum(input_semantics * des_features, dim=-1) # [batch_size, 5, r]\n",
    "            len_input_sen = torch.norm(input_semantics, dim=3) # [batch_size, 5, r]\n",
    "            len_des = torch.norm(des_features, dim=2) # [5, r]\n",
    "            \n",
    "            semantic_scores_norm = semantic_scores / (len_input_sen * len_des) # [batch_size, 5, r]\n",
    "\n",
    "            class_scores = torch.sum(semantic_scores, dim=-1).cpu() # [batch_size, 5]\n",
    "            class_scores_norm = torch.sum(semantic_scores_norm, dim=-1).cpu() # [batch_size, 5]\n",
    "            #''' for semantic score\n",
    "            \n",
    "            \n",
    "            #''' for train representative\n",
    "            train_semantics = model.semantic_features.detach().cpu() # [batch_size, r, hidden_size]\n",
    "            #''' for train representative\n",
    "            for i in range(len(target)):\n",
    "                score_threshold[target[i]] += class_scores[i][train_target[i]]\n",
    "                score_threshold_norm[train_target[i]] += class_scores_norm[i][train_target[i]]\n",
    "                \n",
    "                train_represent[target[i]] += train_semantics[i]\n",
    "                target_count[target[i]] += 1\n",
    "\n",
    "        score_threshold = [x/y for x,y in zip(score_threshold, target_count)]\n",
    "        score_threshold_norm = [x/y for x,y in zip(score_threshold_norm, target_count)]\n",
    "        train_represent = torch.stack([x/y for x,y in zip(train_represent, target_count)])\n",
    "        #'''\n",
    "        \n",
    "        ''' average score for all samples\n",
    "        score_threshold = 0\n",
    "        score_threshold_norm = 0\n",
    "        target_count = 0\n",
    "        for i in range(len(train_target)):\n",
    "            score_threshold += class_scores[i][train_target[i]]\n",
    "            score_threshold_norm += class_scores_norm[i][train_target[i]]\n",
    "            \n",
    "            target_count += 1\n",
    "        \n",
    "        score_threshold /= target_count\n",
    "        score_threshold_norm /= target_count\n",
    "        '''\n",
    "        \n",
    "        ''' min score as threshold for each class\n",
    "        score_threshold = [float(\"inf\")] * n_seen\n",
    "        score_threshold_norm = [float(\"inf\")] * n_seen\n",
    "        for i in range(len(train_target)):\n",
    "            if class_scores[i][train_target[i]] < score_threshold[train_target[i]]:\n",
    "                score_threshold[train_target[i]] = class_scores[i][train_target[i]]\n",
    "            if class_scores_norm[i][train_target[i]] < score_threshold_norm[train_target[i]]:\n",
    "                score_threshold_norm[train_target[i]] = class_scores_norm[i][train_target[i]]\n",
    "        '''\n",
    "\n",
    "        #print(\"Train Max:\", train_max)\n",
    "        acc = accuracy_score(train_target, train_pred)\n",
    "\n",
    "        train_epoch_time = time.time() - start_time\n",
    "        train_time += train_epoch_time\n",
    "\n",
    "        print(\"Epoch: {}\\t| Loss: {}\\t| Acc: {}%\".format(epoch, round(epoch_loss, 4), round(acc * 100., 2)))\n",
    "        print(\"Epoch Time: {}s\\t| Overall Train Time: {}\\n\\n\".format(round(train_epoch_time, 2), datetime.timedelta(seconds=int(train_time))))\n",
    "        test_acc, test_epoch_time = test(epoch, model, test_loader, similarity, n_seen, n_unseen, log_dict, test_set.description_test, score_threshold, score_threshold_norm, train_represent, args.eval_mode)\n",
    "        test_time += test_epoch_time\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        if test_acc > best_acc:\n",
    "            best_acc = test_acc\n",
    "            print(\"Saving best model at epoch {}\".format(epoch))\n",
    "            torch.save(model.state_dict(), args.save_path + \"best_model.pt\") # save best model\n",
    "\n",
    "        if (epoch % args.save_per_epoch) == 0:\n",
    "            print(\"Saving model at epoch {}\".format(epoch))\n",
    "            torch.save(model.state_dict(), args.save_path + \"{}.pt\".format(str(epoch)))\n",
    "\n",
    "        print(\"test_acc: {}\".format(test_acc))\n",
    "        print(\"best_acc: {}\\n\".format(best_acc))\n",
    "        \n",
    "        print(\"Test Time: {}s\\t| Overall Test Time: {}\\n\\n\".format(round(test_epoch_time, 2), datetime.timedelta(seconds=int(test_time))))\n",
    "    \n",
    "    # write log\n",
    "    log_dict['best_acc'] = best_acc\n",
    "    log.write_log(args, log_dict)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0547e1a5",
   "metadata": {},
   "source": [
    "## Testing other stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77881c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([0.9, 0.3, 0.5, 0.2, 0.1])\n",
    "b = [0, 2, 1, 3, 4]\n",
    "print(torch.topk(a, 5)[1])\n",
    "if torch.topk(a, 5)[1].tolist() == b:\n",
    "    print(\"HI\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd93a3a1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.rand(10, 3, 5)\n",
    "b = torch.rand(7, 3, 5)\n",
    "\n",
    "c = torch.unsqueeze(a, 1).repeat(1, 7, 1, 1)\n",
    "print(c.shape)\n",
    "d = torch.sum(b*c, dim=-1)\n",
    "print(d.shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e05986",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.rand(2, 3, 4)\n",
    "b = torch.zeros(3, 4)\n",
    "b += 1\n",
    "c = torch.zeros(2, 3, 4)\n",
    "print(a)\n",
    "print(a*b)\n",
    "print(a*c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86696bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "a = torch.rand(10, 7, 3)\n",
    "print(torch.argmax(a, dim=1).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e1b5b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.Tensor([1, 1])\n",
    "b = torch.clone(a)\n",
    "print(a)\n",
    "print(b)\n",
    "a[0] = 2\n",
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc839c82",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.LongTensor([0]*7)\n",
    "print(a)\n",
    "a[3] += 1\n",
    "print(a)\n",
    "print(torch.max(a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07992909",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [1, 2, 3, 4]\n",
    "b = [1, 2, 3, 4]\n",
    "c = [aa/bb for aa,bb in zip(a,b)]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73551fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros(5, 3, 768)\n",
    "b = torch.randn(3, 768)\n",
    "print(b)\n",
    "a[0] += b\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275ec92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.randn(5, 3, 768)\n",
    "print(a)\n",
    "b = [2, 3, 4, 5, 6]\n",
    "\n",
    "a = torch.stack([x/y for x,y in zip(a, b)])\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa6934bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbac8a71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import time\n",
    "import datetime\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "# 超參數設置\n",
    "EPOCHS = 10\n",
    "BATCH_SIZE = 10\n",
    "PRINT_TIME_PER_STEP = 40\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 1e-2\n",
    "EPSILON = 1e-8\n",
    "\n",
    "LREC_label = ['app', 'bus', 'calc', 'chat', 'cinemas', 'contacts', 'cookbook', 'datetime', 'email', 'epg', 'flight', 'health', \n",
    "              'lottery', 'map', 'match', 'message', 'music', 'news', 'novel', 'poetry', 'radio', 'riddle', 'schedule', 'stock', \n",
    "              'telephone', 'train', 'translation', 'tvchannel', 'video', 'weather', 'website']\n",
    "\n",
    "LREC_train_size = [53, 24, 24, 455, 24, 30, 269, 18, 24, 107, 62, 55,\n",
    "                   24, 68, 24, 63, 66, 58, 24, 102, 24, 34, 29, 71,\n",
    "                   63, 70, 61, 71, 182, 66, 54]\n",
    "\n",
    "LREC_test_size = [18, 8, 8, 51, 8, 10, 90, 6, 8, 36, 21, 18,\n",
    "                  8, 23, 8, 21, 22, 19, 8, 34, 8, 11, 10, 24,\n",
    "                  21, 23, 20, 24, 61, 22, 18]\n",
    "\n",
    "LREC_develop_size = [18, 8, 8, 154, 8, 10, 89, 6, 8, 36, 21, 19,\n",
    "                     8, 23, 8, 21, 22, 20, 8, 34, 8, 11, 9, 24,\n",
    "                     21, 24, 21, 23, 60, 22, 18]\n",
    " \n",
    "# classes: 31\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c970944f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 讀檔\n",
    "def readfile(filename):\n",
    "    with open(filename, encoding=\"utf-8-sig\") as f:\n",
    "        content = f.readlines()\n",
    "        return content\n",
    "    \n",
    "train_path = '/BERT/dataset/text_cls/train/'\n",
    "test_path = '/BERT/dataset/text_cls/test/'\n",
    "predict_path = '/BERT/dataset/text_cls/develop/'\n",
    "\n",
    "# 檔名依字典排序排好\n",
    "train_files = os.listdir(train_path)\n",
    "train_files.sort(key=lambda x:str(x[:-4]))\n",
    "\n",
    "test_files = os.listdir(test_path)\n",
    "test_files.sort(key=lambda x:str(x[:-4]))\n",
    "\n",
    "predict_files = os.listdir(predict_path)\n",
    "predict_files.sort(key=lambda x:str(x[:-4]))\n",
    "\n",
    "# 讀入資料，設定標籤\n",
    "def get_text_and_label(filepath, files, zero_shot_cls = []):\n",
    "    total_text = []\n",
    "    target = []\n",
    "    for i in range(len(files)):\n",
    "        if i in zero_shot_cls:\n",
    "            continue\n",
    "        current_text = readfile(filepath + files[i])\n",
    "        total_text.append(current_text)\n",
    "\n",
    "        current_target = np.full(len(current_text), i)\n",
    "        target = np.concatenate((target, current_target), axis=0)\n",
    "\n",
    "    target = target.reshape(-1, 1)\n",
    "    total_target = torch.tensor(target)\n",
    "    return total_text, total_target\n",
    "\n",
    "zero_shot_cls = []\n",
    "\n",
    "train_text, train_target = get_text_and_label(train_path, train_files, zero_shot_cls)\n",
    "test_text, test_target = get_text_and_label(test_path, test_files, zero_shot_cls)\n",
    "predict_text, predict_target = get_text_and_label(predict_path, predict_files)\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-chinese', cache_dir = \"transformer_file/\")\n",
    "\n",
    "# 測試輸出\n",
    "\n",
    "# training sen: 2299\n",
    "# training sen Max size: 49\n",
    "\n",
    "# testing sen: 667\n",
    "# testing sen Max size: 27"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b48a6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 把每一句轉成數字，每具長度都是64 (62 + 首尾2個特殊 token)\n",
    "def convert_text_to_token(tokenizer, input_text, limit_size = 62):\n",
    "    tokens = tokenizer.encode(input_text[:limit_size])\n",
    "    if len(tokens) < limit_size + 2:\n",
    "        tokens.extend([0] * (limit_size + 2 - len(tokens)))\n",
    "    return tokens\n",
    "\n",
    "train_ids = []\n",
    "for i in range(len(train_text)):\n",
    "    for j in range(len(train_text[i])):\n",
    "        train_ids.append(convert_text_to_token(tokenizer, train_text[i][j]))\n",
    "        \n",
    "train_tokens = torch.tensor(train_ids)\n",
    "print(train_tokens.shape)\n",
    "\n",
    "test_ids = []\n",
    "for i in range(len(test_text)):\n",
    "    for j in range(len(test_text[i])):\n",
    "        test_ids.append(convert_text_to_token(tokenizer, test_text[i][j]))\n",
    "        \n",
    "test_tokens = torch.tensor(test_ids)\n",
    "print(test_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c6f46b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 建立mask\n",
    "def attention_masks(input_ids):\n",
    "    atten_masks = []\n",
    "    for seq in input_ids:\n",
    "        seq_mask =  [float(i>0) for i in seq]\n",
    "        atten_masks.append(seq_mask)\n",
    "    return atten_masks\n",
    "\n",
    "train_atten_masks = attention_masks(train_ids)\n",
    "train_attention_tokens = torch.tensor(train_atten_masks)\n",
    "print(train_attention_tokens.shape)\n",
    "\n",
    "test_atten_masks = attention_masks(test_ids)\n",
    "test_attention_tokens = torch.tensor(test_atten_masks)\n",
    "print(test_attention_tokens.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972e4dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Original text:\\n\", train_text[0][0])\n",
    "print(\"Token tensor:\\n\", train_tokens[0])\n",
    "print(\"Attention mask tensor:\\n\", train_attention_tokens[0])\n",
    "print(\"Target label tensor:\\n\", train_target[0])\n",
    "\n",
    "print(\"Original text:\\n\", test_text[0][0])\n",
    "print(\"Token tensor:\\n\", test_tokens[0])\n",
    "print(\"Attention mask tensor:\\n\", test_attention_tokens[0])\n",
    "print(\"Target label tensor:\\n\", test_target[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0230be9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建 DataLoader\n",
    "train_data = TensorDataset(train_tokens, train_attention_tokens, train_target)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = BATCH_SIZE) \n",
    "\n",
    "test_data = TensorDataset(test_tokens, test_attention_tokens, test_target)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = BATCH_SIZE) \n",
    "\n",
    "# 測試輸出\n",
    "for i, (train, mask, label) in enumerate (train_dataloader):\n",
    "    print(train.shape, mask.shape, label.shape)\n",
    "    break\n",
    "print('Train datalodaer length:', len(train_dataloader))\n",
    "for i, (test, mask, label) in enumerate (test_dataloader):\n",
    "    print(test.shape, mask.shape, label.shape)\n",
    "    break\n",
    "print('Test datalodaer length:', len(test_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87beed4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建模型\n",
    "LREC_model = BertForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels = 31) # 共31種分類\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "LREC_model.to(device)\n",
    "\n",
    "# 定義優化器\n",
    "#optimizer = AdamW(LREC_model.parameters(), lr = LEARNING_RATE, eps = EPSILON)\n",
    "\n",
    "# 定義優化器: bias 和 LayerNorm.weight 不用權重衰減\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer = [\n",
    "    {'params': [p for n, p in LREC_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': WEIGHT_DECAY},\n",
    "    {'params': [p for n, p in LREC_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "optimizer = AdamW(optimizer, lr = LEARNING_RATE, eps = EPSILON)\n",
    "\n",
    "# learning rate scheduler\n",
    "epochs = EPOCHS\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412309a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型準確率\n",
    "def cls_acc(preds, labels):\n",
    "    correct = torch.eq(torch.max(preds, dim = 1)[1], labels.flatten()).float()\n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc\n",
    "\n",
    "# 模型運算時間\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds = elapsed_rounded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb40459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練模型\n",
    "def train(model, optimizer):\n",
    "    t0 = time.time()\n",
    "    avg_loss, avg_acc = [], []\n",
    "    \n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % PRINT_TIME_PER_STEP == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('Batch {:>5,} of {:>5,}.  Time: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch[0].long().to(device), batch[1].long().to(device), batch[2].long().to(device)\n",
    "        output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss, logits = output[0], output[1]\n",
    "        \n",
    "        avg_loss.append(loss.item())\n",
    "        \n",
    "        acc = cls_acc(logits, b_labels)\n",
    "        avg_acc.append(acc)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_loss = round(np.array(avg_loss).mean(),3)\n",
    "    avg_acc = round(np.array(avg_acc).mean(),3)\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170abe59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 評估模型\n",
    "def evaluate(model):\n",
    "    avg_acc = []\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_dataloader:\n",
    "            b_input_ids, b_input_mask, b_labels = batch[0].long().to(device), batch[1].long().to(device), batch[2].long().to(device)\n",
    "            \n",
    "            output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            \n",
    "            acc = cls_acc(output[0], b_labels)\n",
    "            avg_acc.append(acc)\n",
    "    avg_acc = round(np.array(avg_acc).mean(),3)\n",
    "    return avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188b3282",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 執行訓練與評估模型\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(LREC_model, optimizer)\n",
    "    print('epoch = {}, train_acc = {}, train_loss = {}'.format(epoch, train_acc, train_loss))\n",
    "    test_acc = evaluate(LREC_model)\n",
    "    print('epoch = {}, test_acc = {}'.format(epoch, test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c417016b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 預測\n",
    "def predict(model, sen):\n",
    "    input_id = convert_text_to_token(tokenizer, sen)\n",
    "    input_token = torch.tensor(input_id).long().to(device)\n",
    "    \n",
    "    atten_mask = [float(i>0) for i in input_id]\n",
    "    attention_token = torch.tensor(atten_mask).long().to(device)\n",
    "    \n",
    "    output = model(input_token.view(1, -1), token_type_ids=None, attention_mask=attention_token.view(1, -1))\n",
    "    return torch.max(output[0], dim=1)[1]\n",
    "\n",
    "# 預測測試\n",
    "error_count = 0\n",
    "total_count = 0\n",
    "\n",
    "cls_result = []\n",
    "\n",
    "for i in range(len(predict_text)):\n",
    "    cls_total = 0\n",
    "    cls_error = 0\n",
    "    for j in range(len(predict_text[i])):\n",
    "        total_count += 1\n",
    "        cls_total += 1\n",
    "        input_sen = predict_text[i][j]\n",
    "        label = predict(LREC_model, input_sen)\n",
    "        if label != i:\n",
    "            error_count += 1\n",
    "            cls_error += 1\n",
    "            #print('Sentence: ', input_sen[:-1], '\\nClass:', int(label), LREC_label[label].ljust(15), 'Correct class:', i, LREC_label[i])\n",
    "    cls_result.append(\"%d / %d\" % (cls_error, cls_total))\n",
    "\n",
    "for i in range(len(cls_result)):\n",
    "    print(\"Class %d errors: %s\" %(i, cls_result[i]))\n",
    "    \n",
    "print(\"\\nTotal errors: %d/%d\" % (error_count, total_count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0e9a9c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46c2c81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predict_text)):\n",
    "    print(len(predict_text[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579dff07",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = [3,6]\n",
    "for i in range(10):\n",
    "    if i in a:\n",
    "        continue\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c65187",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

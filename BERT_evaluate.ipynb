{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20bf6bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 100\n",
    "BATCH_SIZE = 128\n",
    "PRINT_TIME_PER_STEP = 40\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 1e-2\n",
    "EPSILON = 1e-8\n",
    "evaluation_mode = \"new\"\n",
    "# evaluation mode:\n",
    "    # 'normal': no semantic envolved\n",
    "    # 'unseen_first': if max semantic class in unseen, assign\n",
    "    # 'avg': if semantic score of pred < average score of that class, \n",
    "\n",
    "#paths\n",
    "train_class_path = \"./data/SNIPS/class/train_classes.txt\"\n",
    "test_class_path = \"./data/SNIPS/class/test_classes.txt\"\n",
    "train_data_path = \"./data/SNIPS/sample/train_split.csv\"\n",
    "test_data_path = \"./data/SNIPS/sample/test_split.csv\"\n",
    "train_description_path = \"./data/SNIPS/description/train_description_2.txt\"\n",
    "test_description_path = \"./data/SNIPS/description/test_description_2.txt\"\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3ac3c7a",
   "metadata": {},
   "source": [
    "## Process train classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0160fb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process train classes\n",
    "train_class_list = []\n",
    "with open(train_class_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        train_class_list.append(line.split('\\n')[0])\n",
    "\n",
    "train_class_index = {}\n",
    "\n",
    "for i in range(len(train_class_list)):\n",
    "    label = train_class_list[i]\n",
    "\n",
    "    train_class_index[label] = i\n",
    "    \n",
    "n_seen = len(train_class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d6cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_class_list), \"Train Class list:\\n\", train_class_list)\n",
    "print(len(train_class_index), \"Train Class index:\\n\", train_class_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42e964f",
   "metadata": {},
   "source": [
    "## Process test classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bde11e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process test classes\n",
    "test_class_list = []\n",
    "with open(test_class_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        test_class_list.append(line.split('\\n')[0])\n",
    "\n",
    "test_class_index = {}\n",
    "\n",
    "for i in range(len(test_class_list)):\n",
    "    label = test_class_list[i]\n",
    "\n",
    "    test_class_index[label] = i\n",
    "    \n",
    "n_all = len(test_class_list)\n",
    "n_unseen = n_all - n_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fcddfb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_class_list), \"Test Class list:\\n\", test_class_list)\n",
    "print(len(test_class_index), \"Test Class index:\\n\", test_class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c17dd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%d Seen Classes\" % n_seen)\n",
    "print(\"%d Unseen Classes\" % n_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948546e5",
   "metadata": {},
   "source": [
    "## Process train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610581b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process train data\n",
    "def onehot(idx, class_index):\n",
    "    onehot = torch.zeros(len(class_index))\n",
    "    onehot[idx] = 1.\n",
    "    \n",
    "    return onehot\n",
    "\n",
    "train_sentence_list = []\n",
    "train_label_list = []\n",
    "with open(train_data_path, newline='',encoding='windows-1252') as datas:\n",
    "    sentences = csv.reader(datas)\n",
    "    #headers = next(datas)\n",
    "\n",
    "    for sen in sentences:\n",
    "        train_label_list.append(sen[0])\n",
    "        train_sentence_list.append(sen[1])\n",
    "\n",
    "train_onehot_label = []\n",
    "target = []\n",
    "\n",
    "for i in range(len(train_sentence_list)):\n",
    "    train_onehot_label.append(onehot(train_class_index[train_label_list[i]], train_class_index))\n",
    "    target.append(train_class_index[train_label_list[i]])\n",
    "\n",
    "train_target = torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f224186",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_sentence_list), \"Train Sentences\\n\")\n",
    "print(len(train_target), \"Train Targets:\\n\", train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1231a58c",
   "metadata": {},
   "source": [
    "## Process test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe602af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process test data\n",
    "test_sentence_list = []\n",
    "test_label_list = []\n",
    "with open(test_data_path, newline='',encoding='windows-1252') as datas:\n",
    "    sentences = csv.reader(datas)\n",
    "    #headers = next(datas)\n",
    "\n",
    "    for sen in sentences:\n",
    "        test_label_list.append(sen[0])\n",
    "        test_sentence_list.append(sen[1])\n",
    "\n",
    "test_onehot_label = []\n",
    "target = []\n",
    "\n",
    "for i in range(len(test_sentence_list)):\n",
    "    test_onehot_label.append(onehot(test_class_index[test_label_list[i]], test_class_index))\n",
    "    target.append(test_class_index[test_label_list[i]])\n",
    "\n",
    "test_target = torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60198dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_sentence_list), \"Test Sentences\\n\")\n",
    "print(len(test_target), \"Test Targets:\\n\", test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cf10264",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_samples = 0\n",
    "unseen_samples = 0\n",
    "for targets in test_target:\n",
    "    if targets < n_seen:\n",
    "        seen_samples += 1\n",
    "    else:\n",
    "        unseen_samples += 1\n",
    "        \n",
    "print(\"%d Seen Samples\" % seen_samples)\n",
    "print(\"%d Unseen Samples\" % unseen_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb2323cb",
   "metadata": {},
   "source": [
    "## Process train description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e195d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process train description\n",
    "train_description_list = []\n",
    "with open(train_description_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        train_description_list.append(line.split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b364fdf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_description_list), \"Train Descriptions:\\n\", train_description_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "013cc34c",
   "metadata": {},
   "source": [
    "## Process test description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4282291",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process test description\n",
    "test_description_list = []\n",
    "with open(test_description_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        test_description_list.append(line.split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5ba42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_description_list), \"Test Descriptions:\\n\", test_description_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1b79cf",
   "metadata": {},
   "source": [
    "## BERT preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcc8d494",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT preprocess\n",
    "BERT_tokenizer = AutoTokenizer.from_pretrained('./bert-base-uncased', local_files_only=True)\n",
    "\n",
    "train_encoded_dict = BERT_tokenizer(train_sentence_list, padding = True, return_tensors = 'pt')\n",
    "train_input_ids = train_encoded_dict['input_ids']\n",
    "train_token_type_ids = train_encoded_dict['token_type_ids'] # don't need this\n",
    "train_attention_mask = train_encoded_dict['attention_mask']\n",
    "\n",
    "test_encoded_dict = BERT_tokenizer(test_sentence_list, padding = True, return_tensors = 'pt')\n",
    "test_input_ids = test_encoded_dict['input_ids']\n",
    "test_token_type_ids = test_encoded_dict['token_type_ids'] # don't need this\n",
    "test_attention_mask = test_encoded_dict['attention_mask']\n",
    "\n",
    "description_encoded_dict = BERT_tokenizer(test_description_list, padding = True, return_tensors = 'pt')\n",
    "description_input_ids = description_encoded_dict['input_ids']\n",
    "description_token_type_ids = description_encoded_dict['token_type_ids'] # don't need this\n",
    "description_attention_mask = description_encoded_dict['attention_mask']\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(train_input_ids, train_attention_mask, train_target)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = BATCH_SIZE) \n",
    "\n",
    "test_data = TensorDataset(test_input_ids, test_attention_mask, test_target)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = BATCH_SIZE) \n",
    "\n",
    "for i, (train, mask, label) in enumerate (train_dataloader):\n",
    "    print(train.shape, mask.shape, label.shape)\n",
    "    break\n",
    "print('Train datalodaer length:', len(train_dataloader))\n",
    "for i, (test, mask, label) in enumerate (test_dataloader):\n",
    "    print(test.shape, mask.shape, label.shape)\n",
    "    break\n",
    "print('Test datalodaer length:', len(test_dataloader))\n",
    "\n",
    "print(description_input_ids.shape, description_token_type_ids.shape, description_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5848c09",
   "metadata": {},
   "source": [
    "## BERT sequence classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8de03ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "BERT_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5, output_hidden_states=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "BERT_model.to(device)\n",
    "\n",
    "# Define optimizer: Exclude weight decay for bias and LayerNorm.weight\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_weight_decay = [\n",
    "    {'params': [p for n, p in BERT_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': WEIGHT_DECAY},\n",
    "    {'params': [p for n, p in BERT_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "optimizer = AdamW(optimizer_weight_decay, lr = LEARNING_RATE, eps = EPSILON)\n",
    "\n",
    "# learning rate scheduler\n",
    "epochs = EPOCHS\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc0b64ae",
   "metadata": {},
   "source": [
    "## Accuracy and time evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c010bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def cls_acc(preds, labels):         \n",
    "    correct = torch.eq(torch.max(preds, dim = 1)[1], labels.flatten()).float()\n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc\n",
    "\n",
    "# time\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds = elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba4475dc",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8cc52e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "def train(model, optimizer):\n",
    "    t0 = time.time()\n",
    "    avg_loss, avg_acc = [], []\n",
    "    \n",
    "    score_threshold = [0] * n_seen\n",
    "    score_threshold_norm = [0] * n_seen\n",
    "    target_count = [0] * n_seen\n",
    "    \n",
    "    d_input_ids, d_input_mask = description_input_ids.to(device), description_attention_mask.to(device)\n",
    "    description_output = model(d_input_ids, token_type_ids=None, attention_mask=d_input_mask)\n",
    "    \n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % PRINT_TIME_PER_STEP == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('Batch {:>5,} of {:>5,}.  Time: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch[0].long().to(device), batch[1].long().to(device), batch[2].long().to(device)\n",
    "        output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss, logits, hidden_state = output[0], output[1], output[2]\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        input_compare = torch.unsqueeze(hidden_state[-1][:, 0, :], 1).repeat(1, n_all, 1).detach() # [batch_size, n_seen, 768]\n",
    "        description_compare = description_output[1][-1][:, 0, :].detach() # [n_seen, 768]\n",
    "        compare_score = torch.sum(input_compare * description_compare, dim=-1) # [batch_size, n_seen]\n",
    "        \n",
    "        input_len = torch.norm(input_compare, dim=-1)\n",
    "        des_len = torch.norm(description_compare, dim=-1)\n",
    "        compare_score_norm = (compare_score / (input_len * des_len)).cpu()\n",
    "        compare_score = compare_score.cpu()\n",
    "        \n",
    "        for i in range(len(b_labels)):\n",
    "            score_threshold[b_labels[i]] += compare_score[i][b_labels[i]]\n",
    "            score_threshold_norm[b_labels[i]] += compare_score_norm[i][b_labels[i]]\n",
    "            target_count[b_labels[i]] += 1\n",
    "        \n",
    "        avg_loss.append(loss.item())\n",
    "        \n",
    "        acc = cls_acc(logits, b_labels)\n",
    "        avg_acc.append(acc)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_loss = round(np.array(avg_loss).mean(),3)\n",
    "    avg_acc = round(np.array(avg_acc).mean(),3)\n",
    "    score_threshold = [x/y for x,y in zip(score_threshold, target_count)]\n",
    "    score_threshold_norm = [x/y for x,y in zip(score_threshold_norm, target_count)]\n",
    "    return avg_loss, avg_acc, score_threshold, score_threshold_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac52d69",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abab877",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "def evaluate(model, score_threshold, score_threshold_norm):\n",
    "    avg_acc = []\n",
    "    model.eval()\n",
    "    test_pred = torch.LongTensor([])\n",
    "    compare_score = torch.DoubleTensor([])\n",
    "    compare_score_norm = torch.DoubleTensor([])\n",
    "    with torch.no_grad():\n",
    "        d_input_ids, d_input_mask = description_input_ids.to(device), description_attention_mask.to(device)\n",
    "        description_output = model(d_input_ids, token_type_ids=None, attention_mask=d_input_mask)\n",
    "        \n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        for batch in test_dataloader:\n",
    "            b_input_ids, b_input_mask, b_labels = batch[0].long().to(device), batch[1].long().to(device), batch[2].long().to(device)\n",
    "            output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            \n",
    "            pred = torch.argmax(output[0], dim=1).cpu()\n",
    "            \n",
    "            test_pred = torch.cat((test_pred, pred))\n",
    "            \n",
    "            \n",
    "            # output[0]: logits\n",
    "            # output[1]: hidden_state (13-length tuple, output of embedding layer + 12 layers)\n",
    "            #     output[1][-1][:, 0, :]: last layer [\"CLS\"] token output, shape:[batch_size, 768]\n",
    "            input_compare = torch.unsqueeze(output[1][-1][:, 0, :], 1).repeat(1, n_all, 1) # [batch_size, n_all, 768]\n",
    "            description_compare = description_output[1][-1][:, 0, :] # [n_all, 768]\n",
    "            compare_score_batch = torch.sum(input_compare * description_compare, dim=-1) # [batch_size, n_all]\n",
    "            \n",
    "            input_len = torch.norm(input_compare, dim=-1)\n",
    "            des_len = torch.norm(description_compare, dim=-1)\n",
    "            compare_score_norm_batch = (compare_score_batch / (input_len * des_len)).cpu()\n",
    "            compare_score_batch = compare_score_batch.cpu()\n",
    "            \n",
    "            compare_score = torch.cat((compare_score, compare_score_batch))\n",
    "            compare_score_norm = torch.cat((compare_score_norm, compare_score_norm_batch))\n",
    "            \n",
    "            #acc = cls_acc(output[0], b_labels)\n",
    "            #avg_acc.append(acc)\n",
    "           \n",
    "    test_pred_norm = torch.clone(test_pred)\n",
    "    \n",
    "    correct_point = [0, 0, 0, 0, 0]\n",
    "    wrong_point = [0, 0, 0, 0, 0]\n",
    "    \n",
    "    for i in range(len(test_target)):\n",
    "        argmax_class = torch.argmax(compare_score[i])\n",
    "        argmax_class_norm = torch.argmax(compare_score_norm[i])\n",
    "        \n",
    "        if evaluation_mode == 'normal':\n",
    "            pass\n",
    "        elif evaluation_mode == 'unseen_first':\n",
    "            if argmax_class >= n_seen:\n",
    "                test_pred[i] = argmax_class\n",
    "            if argmax_class_norm >= n_seen:\n",
    "                test_pred_norm[i] = argmax_class_norm\n",
    "        elif evaluation_mode == 'avg':\n",
    "            if compare_score[i][test_pred[i]] < score_threshold[test_pred[i]]:\n",
    "                unseen_score = torch.clone(compare_score[i])\n",
    "                unseen_score[:n_seen] = 0\n",
    "                test_pred[i] = torch.argmax(unseen_score)\n",
    "            if compare_score_norm[i][test_pred_norm[i]] < score_threshold_norm[test_pred_norm[i]]:\n",
    "                unseen_score = torch.clone(compare_score_norm[i])\n",
    "                unseen_score[:n_seen] = 0\n",
    "                test_pred_norm[i] = torch.argmax(unseen_score)\n",
    "        elif evaluation_mode == 'new':\n",
    "            if test_pred[i] != argmax_class:\n",
    "                if compare_score[i][test_pred[i]] < score_threshold[test_pred[i]]:\n",
    "                    if argmax_class >= n_seen:\n",
    "                        test_pred[i] = argmax_class\n",
    "                        # [2]: argmax in unseen\n",
    "                        if test_pred[i] == test_target[i]:\n",
    "                            correct_point[2] += 1\n",
    "                        else:\n",
    "                            wrong_point[2] += 1\n",
    "                            \n",
    "                    else:\n",
    "                        if compare_score[i][argmax_class] >= score_threshold[argmax_class]:\n",
    "                            test_pred[i] = argmax_class\n",
    "                            # [3]: score of argmax >= threshold\n",
    "                            if test_pred[i] == test_target[i]:\n",
    "                                correct_point[3] += 1\n",
    "                            else:\n",
    "                                wrong_point[3] += 1\n",
    "                        else:\n",
    "                            unseen_class_scores = torch.clone(compare_score[i])\n",
    "                            unseen_class_scores[:n_seen] = 0\n",
    "                            test_pred[i] = torch.argmax(unseen_class_scores)\n",
    "                            # [4]: score of pred, argmax < threshold -> unseen\n",
    "                            if test_pred[i] == test_target[i]:\n",
    "                                correct_point[4] += 1\n",
    "                            else:\n",
    "                                wrong_point[4] += 1\n",
    "                else:\n",
    "                    # [1]: score of pred >= threshold\n",
    "                    if test_pred[i] == test_target[i]:\n",
    "                        correct_point[1] += 1\n",
    "                    else:\n",
    "                        wrong_point[1] += 1\n",
    "            else:\n",
    "                # [0]: pred == argmax_class\n",
    "                if test_pred[i] == test_target[i]:\n",
    "                    correct_point[0] += 1\n",
    "                else:\n",
    "                    wrong_point[0] += 1\n",
    "                            \n",
    "            if test_pred_norm[i] != argmax_class_norm:\n",
    "                if compare_score_norm[i][test_pred_norm[i]] < score_threshold_norm[test_pred_norm[i]]:\n",
    "                    if argmax_class_norm >= n_seen:\n",
    "                        test_pred_norm[i] = argmax_class_norm\n",
    "                    else:\n",
    "                        if compare_score_norm[i][argmax_class_norm] >= score_threshold_norm[argmax_class_norm]:\n",
    "                            test_pred_norm[i] = argmax_class_norm\n",
    "                        else:\n",
    "                            unseen_class_scores = torch.clone(compare_score_norm[i])\n",
    "                            unseen_class_scores[:n_seen] = 0\n",
    "                            test_pred_norm[i] = torch.argmax(unseen_class_scores)\n",
    "   \n",
    "        \n",
    "    print(classification_report(test_target, test_pred, digits=4, zero_division=1))\n",
    "    \n",
    "    print(\"Correct Point:\", correct_point)\n",
    "    print(\"Wrong Point:\", wrong_point, \"\\n\")\n",
    "    \n",
    "    test_acc = accuracy_score(test_target, test_pred)\n",
    "    norm_acc = accuracy_score(test_target, test_pred_norm)\n",
    "    print(\"norm_acc = {}\".format(norm_acc))\n",
    "    #avg_acc = round(np.array(avg_acc).mean(),3)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b1c006",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c75608",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Main : train and evaluate the model\n",
    "best_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc, score_threshold, score_threshold_norm = train(BERT_model, optimizer)\n",
    "    print('epoch = {}, train_acc = {}, train_loss = {}\\n'.format(epoch, train_acc, train_loss))\n",
    "    test_acc = evaluate(BERT_model, score_threshold, score_threshold_norm)\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "    print('epoch = {}, test_acc = {}, best_acc = {}\\n'.format(epoch, test_acc, best_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a76fdd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

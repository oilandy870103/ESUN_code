{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9830e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, BertForSequenceClassification\n",
    "from transformers import AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.nn.utils import clip_grad_norm_\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 64\n",
    "PRINT_TIME_PER_STEP = 40\n",
    "LEARNING_RATE = 2e-5\n",
    "WEIGHT_DECAY = 1e-2\n",
    "EPSILON = 1e-8\n",
    "\n",
    "#paths\n",
    "train_class_path = \"./data/SNIPS/class/train_classes.txt\"\n",
    "test_class_path = \"./data/SNIPS/class/test_classes.txt\"\n",
    "train_data_path = \"./data/SNIPS/sample/train_split.csv\"\n",
    "test_data_path = \"./data/SNIPS/sample/test_split.csv\"\n",
    "test_description_path = \"./data/SNIPS/description/test_description_2.txt\"\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63480372",
   "metadata": {},
   "source": [
    "## Process train classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f41b716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process train classes\n",
    "train_class_list = []\n",
    "with open(train_class_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        train_class_list.append(line.split('\\n')[0])\n",
    "\n",
    "train_class_index = {}\n",
    "\n",
    "for i in range(len(train_class_list)):\n",
    "    label = train_class_list[i]\n",
    "\n",
    "    train_class_index[label] = i\n",
    "    \n",
    "n_seen = len(train_class_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1823bcd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_class_list), \"Train Class list:\\n\", train_class_list)\n",
    "print(len(train_class_index), \"Train Class index:\\n\", train_class_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "512a9fab",
   "metadata": {},
   "source": [
    "## Process test classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55e41b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process test classes\n",
    "test_class_list = []\n",
    "with open(test_class_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        test_class_list.append(line.split('\\n')[0])\n",
    "\n",
    "test_class_index = {}\n",
    "\n",
    "for i in range(len(test_class_list)):\n",
    "    label = test_class_list[i]\n",
    "\n",
    "    test_class_index[label] = i\n",
    "    \n",
    "n_all = len(test_class_list)\n",
    "n_unseen = n_all - n_seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e68a4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_class_list), \"Test Class list:\\n\", test_class_list)\n",
    "print(len(test_class_index), \"Test Class index:\\n\", test_class_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "996442f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%d Seen Classes\" % n_seen)\n",
    "print(\"%d Unseen Classes\" % n_unseen)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd963f35",
   "metadata": {},
   "source": [
    "## Process train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a946b72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process train data\n",
    "def onehot(idx, class_index):\n",
    "    onehot = torch.zeros(len(class_index))\n",
    "    onehot[idx] = 1.\n",
    "    \n",
    "    return onehot\n",
    "\n",
    "train_sentence_list = []\n",
    "train_label_list = []\n",
    "with open(train_data_path, newline='',encoding='windows-1252') as datas:\n",
    "    sentences = csv.reader(datas)\n",
    "    #headers = next(datas)\n",
    "\n",
    "    for sen in sentences:\n",
    "        train_label_list.append(sen[0])\n",
    "        train_sentence_list.append(sen[1])\n",
    "\n",
    "train_onehot_label = []\n",
    "target = []\n",
    "\n",
    "for i in range(len(train_sentence_list)):\n",
    "    train_onehot_label.append(onehot(train_class_index[train_label_list[i]], train_class_index))\n",
    "    target.append(train_class_index[train_label_list[i]])\n",
    "\n",
    "train_target = torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0f1560",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(train_sentence_list), \"Train Sentences\\n\")\n",
    "print(len(train_target), \"Train Targets:\\n\", train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4db2695",
   "metadata": {},
   "source": [
    "## Process test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a8ff5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process test data\n",
    "test_sentence_list = []\n",
    "test_label_list = []\n",
    "with open(test_data_path, newline='',encoding='windows-1252') as datas:\n",
    "    sentences = csv.reader(datas)\n",
    "    #headers = next(datas)\n",
    "\n",
    "    for sen in sentences:\n",
    "        test_label_list.append(sen[0])\n",
    "        test_sentence_list.append(sen[1])\n",
    "\n",
    "test_onehot_label = []\n",
    "target = []\n",
    "\n",
    "for i in range(len(test_sentence_list)):\n",
    "    test_onehot_label.append(onehot(test_class_index[test_label_list[i]], test_class_index))\n",
    "    target.append(test_class_index[test_label_list[i]])\n",
    "\n",
    "test_target = torch.tensor(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbc074a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_sentence_list), \"Test Sentences\\n\")\n",
    "print(len(test_target), \"Test Targets:\\n\", test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4fac12",
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_samples = 0\n",
    "unseen_samples = 0\n",
    "for targets in test_target:\n",
    "    if targets < n_seen:\n",
    "        seen_samples += 1\n",
    "    else:\n",
    "        unseen_samples += 1\n",
    "        \n",
    "print(\"%d Seen Samples\" % seen_samples)\n",
    "print(\"%d Unseen Samples\" % unseen_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b025c5fb",
   "metadata": {},
   "source": [
    "## Process test description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f0f511",
   "metadata": {},
   "outputs": [],
   "source": [
    "# process test description\n",
    "test_description_list = []\n",
    "with open(test_description_path, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    for line in lines:\n",
    "        test_description_list.append(line.split('\\n')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979f8151",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(test_description_list), \"Test Descriptions:\\n\", test_description_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5997053d",
   "metadata": {},
   "source": [
    "## BERT preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de4ea36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BERT preprocess\n",
    "BERT_tokenizer = AutoTokenizer.from_pretrained('./bert-base-uncased', local_files_only=True)\n",
    "\n",
    "train_encoded_dict = BERT_tokenizer(train_sentence_list, padding = True, return_tensors = 'pt')\n",
    "train_input_ids = train_encoded_dict['input_ids']\n",
    "train_token_type_ids = train_encoded_dict['token_type_ids'] # don't need this\n",
    "train_attention_mask = train_encoded_dict['attention_mask']\n",
    "\n",
    "test_encoded_dict = BERT_tokenizer(test_sentence_list, padding = True, return_tensors = 'pt')\n",
    "test_input_ids = test_encoded_dict['input_ids']\n",
    "test_token_type_ids = test_encoded_dict['token_type_ids'] # don't need this\n",
    "test_attention_mask = test_encoded_dict['attention_mask']\n",
    "\n",
    "description_encoded_dict = BERT_tokenizer(test_description_list, padding = True, return_tensors = 'pt')\n",
    "description_input_ids = description_encoded_dict['input_ids']\n",
    "description_token_type_ids = description_encoded_dict['token_type_ids'] # don't need this\n",
    "description_attention_mask = description_encoded_dict['attention_mask']\n",
    "\n",
    "# Create DataLoader\n",
    "train_data = TensorDataset(train_input_ids, train_attention_mask, train_target)\n",
    "train_sampler = RandomSampler(train_data)\n",
    "train_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = BATCH_SIZE) \n",
    "\n",
    "test_data = TensorDataset(test_input_ids, test_attention_mask, test_target)\n",
    "test_sampler = SequentialSampler(test_data)\n",
    "test_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = BATCH_SIZE) \n",
    "\n",
    "for i, (train, mask, label) in enumerate (train_dataloader):\n",
    "    print(train.shape, mask.shape, label.shape)\n",
    "    break\n",
    "print('Train datalodaer length:', len(train_dataloader))\n",
    "for i, (test, mask, label) in enumerate (test_dataloader):\n",
    "    print(test.shape, mask.shape, label.shape)\n",
    "    break\n",
    "print('Test datalodaer length:', len(test_dataloader))\n",
    "\n",
    "print(description_input_ids.shape, description_token_type_ids.shape, description_attention_mask.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb1695d",
   "metadata": {},
   "source": [
    "## BERT sequence classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ccff905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "BERT_model = BertForSequenceClassification.from_pretrained(\"bert-base-uncased\", num_labels=5, output_hidden_states=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "BERT_model.to(device)\n",
    "\n",
    "# Define optimizer: Exclude weight decay for bias and LayerNorm.weight\n",
    "no_decay = ['bias', 'LayerNorm.weight']\n",
    "optimizer_weight_decay = [\n",
    "    {'params': [p for n, p in BERT_model.named_parameters() if not any(nd in n for nd in no_decay)], 'weight_decay': WEIGHT_DECAY},\n",
    "    {'params': [p for n, p in BERT_model.named_parameters() if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "optimizer = AdamW(optimizer_weight_decay, lr = LEARNING_RATE, eps = EPSILON)\n",
    "\n",
    "# learning rate scheduler\n",
    "epochs = EPOCHS\n",
    "total_steps = len(train_dataloader) * epochs\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = 0, num_training_steps = total_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb57527",
   "metadata": {},
   "source": [
    "## Accuracy and time evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1a88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def cls_acc(preds, labels):         \n",
    "    correct = torch.eq(torch.max(preds, dim = 1)[1], labels.flatten()).float()\n",
    "    acc = correct.sum().item() / len(correct)\n",
    "    return acc\n",
    "\n",
    "# time\n",
    "def format_time(elapsed):\n",
    "    elapsed_rounded = int(round((elapsed)))\n",
    "    return str(datetime.timedelta(seconds = elapsed_rounded))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c004bb7a",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16059f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train model\n",
    "\n",
    "def train(model, optimizer):\n",
    "    t0 = time.time()\n",
    "    avg_loss, avg_acc = [], []\n",
    "    \n",
    "    score_threshold_avg = [0] * n_seen\n",
    "    target_count = [0] * n_seen\n",
    "    \n",
    "    d_input_ids, d_input_mask = description_input_ids.to(device), description_attention_mask.to(device)\n",
    "    description_output = model(d_input_ids, token_type_ids=None, attention_mask=d_input_mask)\n",
    "    \n",
    "    model.train()\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        if step % PRINT_TIME_PER_STEP == 0 and not step == 0:\n",
    "            elapsed = format_time(time.time() - t0)\n",
    "            print('Batch {:>5,} of {:>5,}.  Time: {:}.'.format(step, len(train_dataloader), elapsed))\n",
    "        \n",
    "        b_input_ids, b_input_mask, b_labels = batch[0].long().to(device), batch[1].long().to(device), batch[2].long().to(device)\n",
    "        output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "        loss, logits, hidden_state = output[0], output[1], output[2]\n",
    "        \n",
    "        input_compare = torch.unsqueeze(hidden_state[-1][:, 0, :], 1).repeat(1, n_all, 1).detach() # [batch_size, n_seen, 768]\n",
    "        description_compare = description_output[1][-1][:, 0, :].detach() # [n_seen, 768]\n",
    "        compare_score = torch.sum(input_compare * description_compare, dim=-1).cpu() # [batch_size, n_seen]\n",
    "        \n",
    "        for i in range(len(b_labels)):\n",
    "            score_threshold_avg[b_labels[i]] += compare_score[i][b_labels[i]]\n",
    "            target_count[b_labels[i]] += 1\n",
    "        \n",
    "        avg_loss.append(loss.item())\n",
    "        \n",
    "        acc = cls_acc(logits, b_labels)\n",
    "        avg_acc.append(acc)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        clip_grad_norm_(model.parameters(), 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "    \n",
    "    avg_loss = round(np.array(avg_loss).mean(),3)\n",
    "    avg_acc = round(np.array(avg_acc).mean(),3)\n",
    "    score_threshold_avg = [x/y for x,y in zip(score_threshold_avg, target_count)]\n",
    "    print(target_count, \"!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    return avg_loss, avg_acc, score_threshold_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ccc7aa",
   "metadata": {},
   "source": [
    "## Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d2b3bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "def evaluate(model, score_threshold_avg):\n",
    "    avg_acc = []\n",
    "    model.eval()\n",
    "    test_pred = torch.LongTensor([])\n",
    "    compare_score = torch.DoubleTensor([])\n",
    "    with torch.no_grad():\n",
    "        d_input_ids, d_input_mask = description_input_ids.to(device), description_attention_mask.to(device)\n",
    "        description_output = model(d_input_ids, token_type_ids=None, attention_mask=d_input_mask)\n",
    "        for batch in test_dataloader:\n",
    "            b_input_ids, b_input_mask, b_labels = batch[0].long().to(device), batch[1].long().to(device), batch[2].long().to(device)\n",
    "            \n",
    "            output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "            pred = torch.argmax(output[0], dim=1).cpu()\n",
    "            # output[0]: logits\n",
    "            # output[1]: hidden_state (13-length tuple, output of embedding layer + 12 layers)\n",
    "            #     output[1][-1][:, 0, :]: last layer [\"CLS\"] token output, shape:[batch_size, 768]\n",
    "            input_compare = torch.unsqueeze(output[1][-1][:, 0, :], 1).repeat(1, n_all, 1) # [batch_size, n_all, 768]\n",
    "            description_compare = description_output[1][-1][:, 0, :] # [n_all, 768]\n",
    "            compare_score_batch = torch.sum(input_compare * description_compare, dim=-1).cpu() # [batch_size, n_all]\n",
    "            \n",
    "            test_pred = torch.cat((test_pred, pred))\n",
    "            compare_score = torch.cat((compare_score, compare_score_batch))\n",
    "            \n",
    "            #acc = cls_acc(output[0], b_labels)\n",
    "            #avg_acc.append(acc)\n",
    "           \n",
    "    final_result = np.zeros((n_all, n_all), dtype=int)\n",
    "    \n",
    "    for i in range(len(test_target)):\n",
    "        argmax_class = torch.argmax(compare_score[i])\n",
    "        '''\n",
    "        if argmax_class >= n_seen:\n",
    "            test_pred[i] = argmax_class\n",
    "        '''\n",
    "        #'''\n",
    "        if compare_score[i][test_pred[i]] < score_threshold_avg[test_pred[i]]:\n",
    "            unseen_score = torch.clone(compare_score[i])\n",
    "            unseen_score[:n_seen] = 0\n",
    "            test_pred[i] = torch.argmax(unseen_score)\n",
    "        #'''\n",
    "        \n",
    "        \n",
    "        final_result[test_target[i]][test_pred[i]] += 1\n",
    "   \n",
    "    \n",
    "    \n",
    "        \n",
    "    final_correct = 0\n",
    "    \n",
    "    for i in range(n_all):\n",
    "        print(\"Class {}: {}\".format(i, final_result[i]))\n",
    "        final_correct += final_result[i][i]\n",
    "        \n",
    "    print(\"\\nFinal Correct:\", final_correct, \"\\n\")\n",
    "        \n",
    "        \n",
    "    print(classification_report(test_target, test_pred, digits=4, zero_division=1))\n",
    "    \n",
    "    test_acc = accuracy_score(test_target, test_pred)\n",
    "    \n",
    "    #avg_acc = round(np.array(avg_acc).mean(),3)\n",
    "    return test_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77e88d6",
   "metadata": {},
   "source": [
    "## Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79410ab9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 執行訓練與評估模型\n",
    "best_acc = 0\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc, score_threshold_avg = train(BERT_model, optimizer)\n",
    "    print('epoch = {}, train_acc = {}, train_loss = {}\\n'.format(epoch, train_acc, train_loss))\n",
    "    test_acc = evaluate(BERT_model, score_threshold_avg)\n",
    "    if test_acc > best_acc:\n",
    "        best_acc = test_acc\n",
    "    print('epoch = {}, test_acc = {}, best_acc = {}\\n'.format(epoch, test_acc, best_acc))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e35aff6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
